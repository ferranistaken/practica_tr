{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import math\n",
    "import cv2\n",
    "import tensorflow.keras.models\n",
    "import tensorflow.keras.layers\n",
    "import tensorflow.keras.applications\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, GlobalMaxPooling2D\n",
    "from tensorflow.keras.applications import VGG16\n",
    "import keras\n",
    "from tensorflow.keras.utils import serialize_keras_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(): \n",
    "    input_layer = Input(shape=(120,120,3))\n",
    "    \n",
    "    vgg = VGG16(include_top=False)(input_layer)\n",
    "    #Model de classificació\n",
    "    f1 = GlobalMaxPooling2D()(vgg)\n",
    "    class1 = Dense(2048, activation='relu')(f1) #relu == funció que determina la classe; \n",
    "    class2 = Dense(3, activation='sigmoid')(class1) # sigmoid == funció que determina la presició de la classe\n",
    "\n",
    "    # sigmoid = f(x) = 1/(1+e^-x)\n",
    "    \n",
    "    #Model de localització de coordenades\n",
    "    f2 = GlobalMaxPooling2D()(vgg)\n",
    "    regress1 = Dense(2048, activation='relu')(f2)\n",
    "    regress2 = Dense(4, activation='sigmoid')(regress1)\n",
    "    \n",
    "    detector = Model(inputs=input_layer, outputs=[class2, regress2])\n",
    "    return detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.0001) #li introduïm el decay que hem calculat a l'optimitzador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localization_loss(y_true, yhat):#primer valor: coordenades reals, segon valor: coordenades previstes     \n",
    "    y_true = tf.reshape(y_true, (5, 4))\n",
    "    \n",
    "    delta_coord = tf.reduce_sum(tf.square(y_true[:,:2] - yhat[:,:2])) #diferència dels dos primers valors de cada fila de la matriu\n",
    "                  \n",
    "    h_true = y_true[:,3] - y_true[:,1] #quarta columna d'una matriu - segona columna\n",
    "    w_true = y_true[:,2] - y_true[:,0] #tercera columna - primera\n",
    "\n",
    "    h_pred = yhat[:,3] - yhat[:,1] \n",
    "    w_pred = yhat[:,2] - yhat[:,0] \n",
    "    '''\n",
    "    delta_size = suma dels quadrats de les diferències entre les dimensions originals \n",
    "    i les dimensions reconstruïdes de l'imatge.\n",
    "    '''\n",
    "    delta_size = tf.reduce_sum(tf.square(w_true - w_pred) + tf.square(h_true-h_pred))\n",
    "    return delta_coord + delta_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classloss = tf.keras.losses.CategoricalCrossentropy() #model que fa una classificació binaria \n",
    "regressloss = localization_loss #model que acabem de crear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector(Model):\n",
    "    def __init__(self, fruita, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.base_model = fruita\n",
    "\n",
    "    def compile(self, opt, classloss, localizationloss, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.closs = classloss\n",
    "        self.lloss = localizationloss\n",
    "        self.opt = opt\n",
    "\n",
    "    def train_step(self, batch, **kwargs):\n",
    "        X, y = batch\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            classes, coords = self.base_model(X, training=True)\n",
    "            batch_classloss = self.closs(tf.reshape(y[0], (5, 3)), classes)\n",
    "            batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
    "\n",
    "            total_loss = batch_localizationloss + 0.5 * batch_classloss\n",
    "\n",
    "        grad = tape.gradient(total_loss, self.base_model.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grad, self.base_model.trainable_variables))\n",
    "\n",
    "        return {\"total_loss\": total_loss, \"class_loss\": batch_classloss, \"regress_loss\": batch_localizationloss}\n",
    "\n",
    "    def test_step(self, batch, **kwargs):\n",
    "        X, y = batch\n",
    "\n",
    "        classes, coords = self.base_model(X, training=False)\n",
    "\n",
    "        batch_classloss = self.closs(tf.reshape(y[0], (5, 3)), classes)\n",
    "        batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
    "        total_loss = batch_localizationloss + 0.5 * batch_classloss\n",
    "\n",
    "        return {\"total_loss\": total_loss, \"class_loss\": batch_classloss, \"regress_loss\": batch_localizationloss}\n",
    "\n",
    "    def call(self, X, **kwargs):\n",
    "        return self.base_model(X, **kwargs)\n",
    "    \n",
    "    def get_config(self):\n",
    "        # Get the base configuration\n",
    "        base_config = super().get_config()\n",
    "        \n",
    "        # Serialize the \"fruita\" model and store it in the configuration\n",
    "        fruita_config = serialize_keras_object(self.base_model)\n",
    "        \n",
    "        # Construct the complete configuration dictionary\n",
    "        config = {\n",
    "            \"fruita\": fruita_config,\n",
    "        }\n",
    "        \n",
    "        # Merge the base configuration and the custom configuration\n",
    "        return {**base_config, **config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Detector(detector)\n",
    "model.build(input_shape=(None, 120, 120, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('detector_fruites_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 131ms/step\n",
      "[array([[0.66013235, 0.18595451, 0.6819148 ]], dtype=float32), array([[0.36427766, 0.3583611 , 0.69749475, 0.7194063 ]], dtype=float32)]\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "[array([[0.66013235, 0.18595451, 0.6819148 ]], dtype=float32), array([[0.36427766, 0.3583611 , 0.69749475, 0.7194063 ]], dtype=float32)]\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "[array([[0.66013235, 0.18595451, 0.6819148 ]], dtype=float32), array([[0.36427766, 0.3583611 , 0.69749475, 0.7194063 ]], dtype=float32)]\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "[array([[0.66013235, 0.18595451, 0.6819148 ]], dtype=float32), array([[0.36427766, 0.3583611 , 0.69749475, 0.7194063 ]], dtype=float32)]\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "[array([[0.66013235, 0.18595451, 0.6819148 ]], dtype=float32), array([[0.36427766, 0.3583611 , 0.69749475, 0.7194063 ]], dtype=float32)]\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "[array([[0.66013235, 0.18595451, 0.6819148 ]], dtype=float32), array([[0.36427766, 0.3583611 , 0.69749475, 0.7194063 ]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "cv2.namedWindow(\"Object Detection\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"Object Detection\", 800, 600)  # Adjust the size as needed\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Resize the frame to (120, 120)\n",
    "    frame = cv2.resize(frame, (120, 120))\n",
    "\n",
    "    # Preprocess the frame for your model\n",
    "    input_data = frame.reshape((1, 120, 120, 3)) / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "    # Perform object detection using your TensorFlow model\n",
    "    predictions = model.predict(input_data)\n",
    "    print (predictions)\n",
    "\n",
    "    # Assuming predictions is a tensor with your model's output\n",
    "\n",
    "    # Loop over the predictions\n",
    "    for prediction in predictions:\n",
    "        confidence = prediction[0]\n",
    "\n",
    "        # If confidence is above a certain threshold (e.g., 0.5)\n",
    "        if confidence[0] > 0.9:\n",
    "            # Customize this part based on your model's output format and post-processing\n",
    "            # Draw a bounding box or take further actions as needed\n",
    "            cv2.rectangle(frame, (0, 0), (120, 120), (0, 255, 0), 2)\n",
    "            label = f'Confidence: {confidence:.2f}'\n",
    "            cv2.putText(frame, label, (0, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        elif confidence[1] > 0.9:\n",
    "            # Customize this part based on your model's output format and post-processing\n",
    "            # Draw a bounding box or take further actions as needed\n",
    "            cv2.rectangle(frame, (0, 0), (120, 120), (0, 255, 0), 2)\n",
    "            label = f'Confidence: {confidence:.2f}'\n",
    "            cv2.putText(frame, label, (0, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        elif confidence[2] > 0.9:\n",
    "            # Customize this part based on your model's output format and post-processing\n",
    "            # Draw a bounding box or take further actions as needed\n",
    "            cv2.rectangle(frame, (0, 0), (120, 120), (0, 255, 0), 2)\n",
    "            label = f'Confidence: {confidence:.2f}'\n",
    "            cv2.putText(frame, label, (0, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Object Detection\", frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "# Release the camera and close the OpenCV window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
