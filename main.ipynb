{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7652d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install labelme tensorflow opencv-python matplotlib albumentations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e0455a",
   "metadata": {},
   "source": [
    "# 1. Obtenció i tractament de dades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9578d20b",
   "metadata": {},
   "source": [
    "### 1.1 Ús de la llibreria LabelMe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaed57d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!labelme "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d6376c",
   "metadata": {},
   "source": [
    "###  1.2 Creació de la base de dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b42b8df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importació de llibreries necessàries\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b7ac200",
   "metadata": {},
   "outputs": [],
   "source": [
    "#limitació de la memòria GPU \n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce224428",
   "metadata": {},
   "outputs": [],
   "source": [
    "imatges = tf.data.Dataset.list_files('imatges\\\\*.jpg') # introduïm les imatges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f3a5584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_imatge(imatge): #funció per a carregar imatges\n",
    "    byte_img = tf.io.read_file(imatge)\n",
    "    img = tf.image.decode_jpeg(byte_img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c7a78f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "imatges = imatges.map(carregar_imatge) #executem la funció "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dad48be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[78, 53, 31],\n",
       "        [78, 53, 31],\n",
       "        [79, 52, 33],\n",
       "        ...,\n",
       "        [65, 36, 18],\n",
       "        [63, 36, 19],\n",
       "        [60, 34, 17]],\n",
       "\n",
       "       [[77, 52, 30],\n",
       "        [86, 61, 39],\n",
       "        [75, 48, 29],\n",
       "        ...,\n",
       "        [73, 44, 26],\n",
       "        [71, 44, 25],\n",
       "        [68, 43, 23]],\n",
       "\n",
       "       [[76, 52, 28],\n",
       "        [75, 51, 27],\n",
       "        [72, 47, 25],\n",
       "        ...,\n",
       "        [80, 49, 29],\n",
       "        [82, 54, 33],\n",
       "        [77, 50, 29]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[66, 41, 21],\n",
       "        [67, 42, 22],\n",
       "        [64, 39, 19],\n",
       "        ...,\n",
       "        [78, 57, 40],\n",
       "        [70, 51, 34],\n",
       "        [67, 48, 33]],\n",
       "\n",
       "       [[63, 41, 20],\n",
       "        [69, 47, 26],\n",
       "        [69, 46, 28],\n",
       "        ...,\n",
       "        [69, 48, 31],\n",
       "        [68, 47, 30],\n",
       "        [63, 42, 25]],\n",
       "\n",
       "       [[65, 43, 22],\n",
       "        [67, 45, 24],\n",
       "        [62, 39, 21],\n",
       "        ...,\n",
       "        [68, 46, 32],\n",
       "        [68, 47, 30],\n",
       "        [66, 45, 28]]], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imatges.as_numpy_iterator().next() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa93ae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imatgesDividir(dir_carpeta, n_imatges):\n",
    "    num_elements = len(os.listdir(os.path.join(dir_carpeta, \"imatges\")))\n",
    "    n_train = round(num_elements*0.7) # 70% de les imatges per entrenar\n",
    "    n_test = round(num_elements*0.15) # 15% de les imatges per provar\n",
    "    n_val = num_elements - (n_train + n_test) # 15% de les imatges per validar\n",
    "\n",
    "    llista = []\n",
    "    for i in range(1, (n_imatges + 1)):\n",
    "        llista.append(i)\n",
    "\n",
    "    if (len(os.listdir(os.path.join(dir_carpeta,\"dades\",\"test\")))-1)< n_test:\n",
    "        for i in range(n_test):\n",
    "            n = random.choice(llista)\n",
    "            nom = str(str(n) + \".jpg\")\n",
    "            shutil.move(os.path.join(dir_carpeta, \"imatges\", nom ), os.path.join(dir_carpeta, \"dades\", \"test\" , \"imatges\"))\n",
    "            llista.remove(n)\n",
    "\n",
    "    if (len(os.listdir(os.path.join(dir_carpeta,\"dades\",\"val\")))-1) < n_val:\n",
    "        for i in range(n_val):\n",
    "            n = random.choice(llista)\n",
    "            nom = str(str(n) + \".jpg\")\n",
    "            shutil.move(os.path.join(dir_carpeta, \"imatges\", nom), os.path.join(dir_carpeta,\"dades\" , \"val\", \"imatges\"))\n",
    "            llista.remove(n)\n",
    "\n",
    "    if (os.listdir(os.path.join(dir_carpeta, \"dades\",\"val\"))) >= n_val and (os.listdir(os.path.join(dir_carpeta,\"dades\",\"test\"))) >= n_test: \n",
    "        for i in llista:\n",
    "            nom = str(i) + \".jpg\"\n",
    "            shutil.move(os.path.join(dir_carpeta, \"imatges\", nom), os.path.join(dir_carpeta, \"dades\",\"train\", \"imatges\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8643289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la carpeta principal\n",
    "os.mkdir(\"dades\")\n",
    "\n",
    "# Crear les subcarpetes dins de la carpeta principal\n",
    "for subcarpeta in [\"train\", \"test\", \"val\"]:\n",
    "    path_subcarpeta = os.path.join(\"dades\", subcarpeta)\n",
    "    os.mkdir(path_subcarpeta)\n",
    "    path_subsubcarpeta = os.path.join(\"dades\", subcarpeta, \"imatges\")\n",
    "    os.mkdir(path_subsubcarpeta)\n",
    "    path_subsubcarpeta2 = os.path.join(\"dades\", subcarpeta, \"labels\")\n",
    "    os.mkdir(path_subsubcarpeta2)\n",
    "\n",
    "imatgesDividir('T:\\practica_tr',300) #divisió de les carpetes i de les imatges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b439be43",
   "metadata": {},
   "source": [
    "### Partició de les dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7dce6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mou les els labels a les seves respectives carpetes\n",
    "def moureLabels(dir_carpeta):\n",
    "    for carpeta in ['train','test','val']:\n",
    "        for arxiu in os.listdir(os.path.join(dir_carpeta,'dades', carpeta, 'imatges')): #per cada arxiu en cada carpeta\n",
    "            n = arxiu.split(\".\")\n",
    "            json = str(n[0] + \".json\")\n",
    "            shutil.move(os.path.join(dir_carpeta, \"labels\", json ), os.path.join(dir_carpeta, \"dades\", carpeta, \"labels\"))#canviem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2277dd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "moureLabels('T:\\practica_tr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f70389c",
   "metadata": {},
   "source": [
    "### 1.2.1 Ús de la llibreria Albumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "071c1384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as alb\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93969970",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentor = alb.Compose([alb.RandomCrop(width=1024, height=1024),\n",
    "                         alb.HorizontalFlip(p=0.5), \n",
    "                         alb.RandomBrightnessContrast(p=0.5),\n",
    "                         alb.RandomGamma(p=0.7), \n",
    "                         alb.RGBShift(p=0.5), \n",
    "                         alb.VerticalFlip(p=0.7)], \n",
    "                       bbox_params=alb.BboxParams(format='albumentations', \n",
    "                                                  label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e41c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"aug_dades\")\n",
    "\n",
    "# Crear les subcarpetes dins de la carpeta principal\n",
    "for subcarpeta in [\"train\", \"test\", \"val\"]:\n",
    "    path_subcarpeta = os.path.join(\"aug_dades\", subcarpeta)\n",
    "    os.mkdir(path_subcarpeta)\n",
    "    path_subsubcarpeta = os.path.join(\"aug_dades\", subcarpeta, \"imatges\")\n",
    "    os.mkdir(path_subsubcarpeta)\n",
    "    path_subsubcarpeta2 = os.path.join(\"aug_dades\", subcarpeta, \"labels\")\n",
    "    os.mkdir(path_subsubcarpeta2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a36e346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested crop size (1024, 1024) is larger than the image size (512, 512)\n",
      "Requested crop size (1024, 1024) is larger than the image size (512, 512)\n",
      "Requested crop size (1024, 1024) is larger than the image size (512, 512)\n",
      "Requested crop size (1024, 1024) is larger than the image size (512, 512)\n"
     ]
    }
   ],
   "source": [
    "classes_fruita = [\"buit\",\"poma\", \"pera\", \"mandarina\"]\n",
    "partitions = ['train', 'test', 'val']\n",
    "\n",
    "for partition in partitions:\n",
    "    input_folder = os.path.join('dades', partition, 'imatges')\n",
    "    output_folder = os.path.join('aug_dades', partition, 'imatges')\n",
    "\n",
    "    for imatge in os.listdir(input_folder):\n",
    "        img_path = os.path.join(input_folder, imatge)\n",
    "        img = cv2.imread(img_path)\n",
    "        label_path = os.path.join('dades', partition, 'labels', f'{imatge.split(\".\")[0]}.json')\n",
    "\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                label = json.load(f)\n",
    "\n",
    "            annotations = [] #contindrà la informació de la imatge augmentada \n",
    "\n",
    "            for shape in label['shapes']:\n",
    "                class_name = shape['label'] \n",
    "                class_id = classes_fruita.index(class_name)\n",
    "                coords = shape['points']\n",
    "                coords = [(coords[0][0]/1024), (coords[0][1]/1024), (coords[1][0]/1024), (coords[1][1]/1024)]\n",
    "\n",
    "                try:\n",
    "                    for x in range(1):\n",
    "                        augmented = augmentor(image=img, bboxes=[coords], class_labels=[class_name])\n",
    "                        augmented_img = augmented['image']\n",
    "\n",
    "                        annotation = {\n",
    "                            'image': f'{imatge.split(\".\")[0]}.{x}.jpg',\n",
    "                            'bbox': coords,\n",
    "                            'class': class_name,\n",
    "                        }\n",
    "                        output_img_path = os.path.join(output_folder, annotation['image'])\n",
    "                        cv2.imwrite(output_img_path, augmented_img)\n",
    "\n",
    "                        output_json_path = os.path.join('aug_dades', partition, 'labels', f'{imatge.split(\".\")[0]}.{x}.json')\n",
    "                        with open(output_json_path, 'w') as f:\n",
    "                            json.dump([annotation], f) \n",
    "                except Exception as e:\n",
    "                    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf0c55a",
   "metadata": {},
   "source": [
    "### Incloure imatges creades amb Albumentations a la Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f16bf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obrim les carpetes amb les img a manipular i les carreguem\n",
    "train_images = tf.data.Dataset.list_files('aug_dades\\\\train\\\\imatges\\\\*.jpg',shuffle=False)\n",
    "train_images = train_images.map(carregar_imatge)\n",
    "#Es fa resize de les img i també baixem l'escala de la imatge a 1\n",
    "train_images = train_images.map(lambda x: tf.image.resize(x, (250,250)))\n",
    "train_images = train_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05456564",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = tf.data.Dataset.list_files('aug_dades\\\\test\\\\imatges\\\\*.jpg', shuffle=False)\n",
    "test_images = test_images.map(carregar_imatge)\n",
    "test_images = test_images.map(lambda x: tf.image.resize(x, (250,250)))\n",
    "test_images = test_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "167107c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images = tf.data.Dataset.list_files('aug_dades\\\\val\\\\imatges\\\\*.jpg', shuffle=False)\n",
    "val_images = val_images.map(carregar_imatge)\n",
    "val_images = val_images.map(lambda x: tf.image.resize(x, (250,250)))\n",
    "val_images = val_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "59755dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.5176471 , 0.59822744, 0.48197648],\n",
       "        [0.5187886 , 0.6011415 , 0.4795729 ],\n",
       "        [0.5210099 , 0.60336286, 0.4817942 ],\n",
       "        ...,\n",
       "        [0.5233412 , 0.60177255, 0.4958902 ],\n",
       "        [0.52156866, 0.6       , 0.5019608 ],\n",
       "        [0.52156866, 0.6       , 0.5019608 ]],\n",
       "\n",
       "       [[0.52219963, 0.60455257, 0.48298398],\n",
       "        [0.52156866, 0.6039216 , 0.48235294],\n",
       "        [0.51969975, 0.6020527 , 0.4804841 ],\n",
       "        ...,\n",
       "        [0.5229647 , 0.6013961 , 0.49551374],\n",
       "        [0.5249932 , 0.60342455, 0.5021326 ],\n",
       "        [0.52156866, 0.6       , 0.5019608 ]],\n",
       "\n",
       "       [[0.5294118 , 0.6117647 , 0.49019608],\n",
       "        [0.5254902 , 0.60784316, 0.48554856],\n",
       "        [0.5254902 , 0.60784316, 0.4862745 ],\n",
       "        ...,\n",
       "        [0.5276374 , 0.6060688 , 0.50018644],\n",
       "        [0.5244706 , 0.602902  , 0.49774563],\n",
       "        [0.5244706 , 0.602902  , 0.5048628 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.7058824 , 0.79607844, 0.68235296],\n",
       "        [0.7058824 , 0.79607844, 0.68235296],\n",
       "        [0.7058824 , 0.79607844, 0.68235296],\n",
       "        ...,\n",
       "        [0.69046134, 0.7835591 , 0.6795586 ],\n",
       "        [0.691216  , 0.7872154 , 0.6745098 ],\n",
       "        [0.69177485, 0.7877743 , 0.6750687 ]],\n",
       "\n",
       "       [[0.7072785 , 0.79747456, 0.6837491 ],\n",
       "        [0.70727843, 0.7974745 , 0.683749  ],\n",
       "        [0.7058824 , 0.79607844, 0.6837491 ],\n",
       "        ...,\n",
       "        [0.6955138 , 0.78570986, 0.6837491 ],\n",
       "        [0.69411767, 0.78431374, 0.68235296],\n",
       "        [0.69273376, 0.78292984, 0.68096906]],\n",
       "\n",
       "       [[0.707655  , 0.7978511 , 0.6880472 ],\n",
       "        [0.7090389 , 0.799235  , 0.6894311 ],\n",
       "        [0.707655  , 0.7978511 , 0.6880472 ],\n",
       "        ...,\n",
       "        [0.69355875, 0.7837548 , 0.68179405],\n",
       "        [0.6901961 , 0.779761  , 0.6796935 ],\n",
       "        [0.69099736, 0.77727187, 0.6870758 ]]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0900d8c7",
   "metadata": {},
   "source": [
    "### 2.5 Carregar etiquetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f31855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funció que carrega els labels\n",
    "def carregar_labels(x):\n",
    "    with open(x.numpy(), 'r', encoding='utf-8') as f:\n",
    "        label = json.load(f)\n",
    "    return [label['class']],label['bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f279ef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = tf.data.Dataset.list_files('aug_dades\\\\train\\\\labels\\\\*.json', shuffle=False)\n",
    "train_labels = train_labels.map(lambda x: tf.py_function(carregar_labels, [x], [tf.uint8, tf.float16]))#tf_py_function(funció, paràmetres, tipus de retorn) \n",
    "#obre els labels i els aplica la funció carregar_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2fbc102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = tf.data.Dataset.list_files('aug_dades\\\\test\\\\labels\\\\*.json', shuffle=False)\n",
    "test_labels = test_labels.map(lambda x: tf.py_function(carregar_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a532244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = tf.data.Dataset.list_files('aug_dades\\\\val\\\\labels\\\\*.json', shuffle=False)\n",
    "val_labels = val_labels.map(lambda x: tf.py_function(carregar_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "56daf25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.data.ops.dataset_ops._NumpyIterator at 0x27c301a74f0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c9566b",
   "metadata": {},
   "source": [
    "### Combinar etiquetes i imatges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00975b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 208, Test: 44, Val: 44\n"
     ]
    }
   ],
   "source": [
    "#cal comprovar quantes imatges hi ha en cada carpeta\n",
    "print(\"Train: \" + str(len(train_labels)) + \", Test: \" + str(len(test_labels)) + \", Val: \" + str(len(val_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ea1c80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.data.Dataset.zip((train_images, train_labels)) #mètode que junta les imatges amb els labels\n",
    "train = train.shuffle(250) #posar el número que hi ha apoximat cap amunt, en aquest cas 2000\n",
    "train = train.batch(8)#crear lots de 8\n",
    "train = train.prefetch(4)#redueix la capacitat de procesar per evitar errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0ba0be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tf.data.Dataset.zip((test_images, test_labels))\n",
    "test = test.shuffle(50)\n",
    "test = test.batch(8)\n",
    "test = test.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "910bd91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = tf.data.Dataset.zip((val_images, val_labels))\n",
    "val = val.shuffle(50)\n",
    "val = val.batch(8)\n",
    "val = val.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22d83492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 250, 250, None), dtype=tf.float32, name=None), (TensorSpec(shape=<unknown>, dtype=tf.uint8, name=None), TensorSpec(shape=<unknown>, dtype=tf.float16, name=None)))>\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "18989ef6",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} TypeError: list indices must be integers or slices, not str\nTraceback (most recent call last):\n\n  File \"t:\\env\\Lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 265, in __call__\n    return func(device, token, args)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"t:\\env\\Lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 143, in __call__\n    outputs = self._call(device, args)\n              ^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"t:\\env\\Lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 150, in _call\n    ret = self._func(*args)\n          ^^^^^^^^^^^^^^^^^\n\n  File \"t:\\env\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\odena\\AppData\\Local\\Temp\\ipykernel_11908\\509285390.py\", line 5, in carregar_labels\n    return [label['class']],label['bbox']\n            ~~~~~^^^^^^^^^\n\nTypeError: list indices must be integers or slices, not str\n\n\n\t [[{{node EagerPyFunc}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mt:\\practica_tr\\main.ipynb Cell 38\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/t%3A/practica_tr/main.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train\u001b[39m.\u001b[39;49mas_numpy_iterator()\u001b[39m.\u001b[39;49mnext()[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape\n",
      "File \u001b[1;32mt:\\env\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4665\u001b[0m, in \u001b[0;36m_NumpyIterator.next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   4664\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m-> 4665\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__next__\u001b[39;49m()\n",
      "File \u001b[1;32mt:\\env\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4662\u001b[0m, in \u001b[0;36m_NumpyIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   4659\u001b[0m     numpy\u001b[39m.\u001b[39msetflags(write\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   4660\u001b[0m   \u001b[39mreturn\u001b[39;00m numpy\n\u001b[1;32m-> 4662\u001b[0m \u001b[39mreturn\u001b[39;00m nest\u001b[39m.\u001b[39mmap_structure(to_numpy, \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator))\n",
      "File \u001b[1;32mt:\\env\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:797\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    796\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 797\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_internal()\n\u001b[0;32m    798\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    799\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mt:\\env\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:780\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[39m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[0;32m    778\u001b[0m \u001b[39m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[0;32m    779\u001b[0m \u001b[39mwith\u001b[39;00m context\u001b[39m.\u001b[39mexecution_mode(context\u001b[39m.\u001b[39mSYNC):\n\u001b[1;32m--> 780\u001b[0m   ret \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39;49miterator_get_next(\n\u001b[0;32m    781\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource,\n\u001b[0;32m    782\u001b[0m       output_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_types,\n\u001b[0;32m    783\u001b[0m       output_shapes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_shapes)\n\u001b[0;32m    785\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    786\u001b[0m     \u001b[39m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_spec\u001b[39m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mt:\\env\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3043\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   3041\u001b[0m   \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   3042\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m-> 3043\u001b[0m   _ops\u001b[39m.\u001b[39;49mraise_from_not_ok_status(e, name)\n\u001b[0;32m   3044\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_FallbackException:\n\u001b[0;32m   3045\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[1;32mt:\\env\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7262\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7260\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7261\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 7262\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} TypeError: list indices must be integers or slices, not str\nTraceback (most recent call last):\n\n  File \"t:\\env\\Lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 265, in __call__\n    return func(device, token, args)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"t:\\env\\Lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 143, in __call__\n    outputs = self._call(device, args)\n              ^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"t:\\env\\Lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 150, in _call\n    ret = self._func(*args)\n          ^^^^^^^^^^^^^^^^^\n\n  File \"t:\\env\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\odena\\AppData\\Local\\Temp\\ipykernel_11908\\509285390.py\", line 5, in carregar_labels\n    return [label['class']],label['bbox']\n            ~~~~~^^^^^^^^^\n\nTypeError: list indices must be integers or slices, not str\n\n\n\t [[{{node EagerPyFunc}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "train.as_numpy_iterator().next()[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bdd4d3",
   "metadata": {},
   "source": [
    "# 2. Preparació de la intel·ligència artificial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254f476e",
   "metadata": {},
   "source": [
    "### 2.1 Descarregar i carregar el model VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d709c624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.models\n",
    "import tensorflow.keras.layers\n",
    "import tensorflow.keras.applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87f38e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, GlobalMaxPooling2D\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1cd114",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16(include_top=False) \n",
    "#Marca que les ultimes capes de la xarxa no les utilitzarem perquè afegirem les nostres pròpies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80caed09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None, None, 3)]   0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, None, None, 64)    1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, None, None, 64)    36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, None, None, 64)    0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, None, None, 128)   73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, None, None, 128)   147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, None, None, 128)   0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, None, None, 256)   295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, None, None, 256)   0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, None, None, 512)   1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14714688 (56.13 MB)\n",
      "Trainable params: 14714688 (56.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26209fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "La funció crea i retorna un model de xarxa neuronal convolucional per a la detecció dels objectes. \n",
    "El model té dues parts: una per a determinar si una imatge conté algun objecte i una altra per a localitzar l'obkecte (poma, pera o mandarina) a l'imatge. \n",
    "Al utilitzar VGG16, com que esta pre-entrenada, les característiques del input\n",
    "i les classifica en dues branques que son les dues parts mencionades abans de manera automàtica. \n",
    "'''\n",
    "def build_model(): \n",
    "    input_layer = Input(shape=(250,250,3))\n",
    "    \n",
    "    vgg = VGG16(include_top=False)(input_layer)\n",
    "    #Model de classificació\n",
    "    f1 = GlobalMaxPooling2D()(vgg)\n",
    "    class1 = Dense(2048, activation='relu')(f1) #relu == funció que determina la classe; \n",
    "    class2 = Dense(1, activation='sigmoid')(class1) # sigmoid == funció que determina la presició de la classe\n",
    "\n",
    "    # sigmoid = f(x) = 1/(1+e^-x)\n",
    "    \n",
    "    #Model de localització de coordenades\n",
    "    f2 = GlobalMaxPooling2D()(vgg)\n",
    "    regress1 = Dense(2048, activation='relu')(f2)\n",
    "    regress2 = Dense(4, activation='sigmoid')(regress1)\n",
    "    \n",
    "    detector = Model(inputs=input_layer, outputs=[class2, regress2])\n",
    "    return detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97226aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.as_numpy_iterator().next()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84e6fb2",
   "metadata": {},
   "source": [
    "### 3.3 Prova de la xarxa neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dbda40",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3785c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.summary()\n",
    "'''\n",
    "Aquest fragment dona una descripció resumida de la xarxa que acabem de construir,\n",
    "inclou el nombre de capes, el nombre de paràmetres que ha après durant un entrenament i \n",
    "la forma de les dades que passen entre les capes.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49668c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f4beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715392aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, coords = detector.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586ead66",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e82e086",
   "metadata": {},
   "source": [
    "### 2.2 Funcions de pèrdua i optimitzadors\n",
    "Els optimitzadors són algorismes que s’utilitzen per ajustar els pesos d’una xarxa neuronal durant l’entrenament. Els optimitzadors són responsables de minimitzar la funció de pèrdua de la xarxa neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a95066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001) #li introduïm el decay que hem calculat a l'optimitzador\n",
    "\n",
    "'''\n",
    "En el context de les xarxes neuronals, un optimitzador és un algorisme\n",
    "que ajuda a ajustar els paràmetres de la xarxa per aconseguir una millor precisió.\n",
    "L'optimitzador Adam és un exemple d'això i ajuda a l'optimitzador a convergir més ràpidament\n",
    "i amb més precisió.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bb0627",
   "metadata": {},
   "source": [
    "### Creació de 'Localitzation Loss' i 'Classification Loss'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400ff5f5",
   "metadata": {},
   "source": [
    "La funció té dos components: la pèrdua de localització i la pèrdua de classificació.\n",
    "La pèrdua de localització mesura la diferència entre les coordenades dels quadres delimitadors \n",
    "predits i les coordenades dels quadres delimitadors reals. La pèrdua de classificació mesura la\n",
    "diferència entre les probabilitats de classe predites i les probabilitats de classe reals. \n",
    "En aquesta funció, només es calcula la pèrdua de localització.\n",
    "\n",
    "La funció té com a entrada dos tensors: y_true i yhat. \n",
    "y_true conté les coordenades dels quadres delimitadors reals i\n",
    "les probabilitats de classe reals per a cada objecte en la imatge d’entrada.\n",
    "yhat conté les coordenades dels quadres delimitadors predits i les\n",
    "probabilitats de classe predites per a cada objecte en la imatge d’entrada.\n",
    "\n",
    "La funció calcula la pèrdua de localització sumant el quadrat de la diferència\n",
    "entre les coordenades dels quadres delimitadors reals i les coordenades dels quadres\n",
    "delimitadors predits. A continuació, calcula la diferència entre l’amplada i l’alçada\n",
    "dels quadres delimitadors reals i els quadres delimitadors predits i suma els quadrats\n",
    "d’aquestes diferències. Finalment, retorna la suma de les dues pèrdues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0159efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def localization_loss(y_true, yhat):#primer valor: coordenades reals, segon valor: coordenades previstes     \n",
    "    delta_coord = tf.reduce_sum(tf.square(y_true[:,:2] - yhat[:,:2])) #diferència dels dos primers valors de cada fila de la matriu\n",
    "                  \n",
    "    h_true = y_true[:,3] - y_true[:,1] #quarta columna d'una matriu - segona columna\n",
    "    w_true = y_true[:,2] - y_true[:,0] #tercera columna - primera\n",
    "\n",
    "    h_pred = yhat[:,3] - yhat[:,1] \n",
    "    w_pred = yhat[:,2] - yhat[:,0] \n",
    "    '''\n",
    "    delta_size = suma dels quadrats de les diferències entre les dimensions originals \n",
    "    i les dimensions reconstruïdes de l'imatge.\n",
    "    '''\n",
    "    delta_size = tf.reduce_sum(tf.square(w_true - w_pred) + tf.square(h_true-h_pred))\n",
    "    return delta_coord + delta_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72140c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "classloss = tf.keras.losses.BinaryCrossentropy() #model que fa una classificació binaria \n",
    "regressloss = localization_loss #model que acabem de crear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34a7fc1",
   "metadata": {},
   "source": [
    "### Test de les mètriques de 'loss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9d2d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test, en les 2 cel·les inferiors s'hauria d'obtenir : <tf.Tensor: shape=(), dtype=float32, numpy=n>\n",
    "regressloss(y[1], coords) #y[0] ==> 0 o 1 (hi ha maduixa o no) , y[1] ==> coords (tot del batch sencer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb81dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "classloss(y[0], classes) #numpy ==> probabilitat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ae7b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "localization_loss(y[1],coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130bdab9",
   "metadata": {},
   "source": [
    "# 3. Entrenament de la intel·ligència artificial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fe1595",
   "metadata": {},
   "source": [
    "### 3.1 Creació del propi model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1b2c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector(Model): \n",
    "    def __init__(self, fruita,  **kwargs): \n",
    "        super().__init__(**kwargs)\n",
    "        self.model = fruita\n",
    "\n",
    "    def compile(self, opt, classloss, localizationloss, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.closs = classloss\n",
    "        self.lloss = localizationloss\n",
    "        self.opt = opt\n",
    "    '''\n",
    "    La funció train_step executa cada pas de l'entrenament del model. El model rep un lot de dades\n",
    "    del entrenament (data que hem escollit per a 'train'),i calcula la pèrdua total (total_loss) que \n",
    "    consisteix en la suma de la pèrdua de localització (batch_localizationloss) i la meitat de la\n",
    "    pèrdua de classificació (batch_classloss). Finalment, calcula la perdua total i actualitza els\n",
    "    pesos.\n",
    "    '''\n",
    "    def train_step(self, batch, **kwargs): \n",
    "        \n",
    "        X, y = batch\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            classes, coords = self.model(X, training=True)\n",
    "            \n",
    "            batch_classloss = self.closs(y[0], classes)\n",
    "            batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
    "            \n",
    "            total_loss = batch_localizationloss+0.5*batch_classloss \n",
    "            \n",
    "            grad = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "            #el gradient representa la direcció i la magnitud en la qual s'ha\n",
    "            #d'ajustar cada paràmetre del model per reduir la pèrdua (loss) durant l'entrenament\n",
    "        \n",
    "        opt.apply_gradients(zip(grad, self.model.trainable_variables))\n",
    "        \n",
    "        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regress_loss\":batch_localizationloss}\n",
    "    '''\n",
    "    Aquesta funció 'test_step' agafa la data de 'train' per avaluar-la i torna a calcular la pèrdua\n",
    "    total, la pèrdua de classificació i la pèrdua de localització. Això es fa per avaluar el \n",
    "    rendiment del model en dades noves.\n",
    "    '''\n",
    "    def test_step(self, batch, **kwargs): \n",
    "        X, y = batch\n",
    "        \n",
    "        classes, coords = self.model(X, training=False)\n",
    "        \n",
    "        batch_classloss = self.closs(y[0], classes)\n",
    "        batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
    "        total_loss = batch_localizationloss+0.5*batch_classloss\n",
    "        \n",
    "        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regress_loss\":batch_localizationloss}\n",
    "        \n",
    "    def call(self, X, **kwargs): \n",
    "        return self.model(X, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f405338",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Detector(detector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a834cc1d",
   "metadata": {},
   "source": [
    "El 'mode.compile' configura el model per utilitzar l'optimitzador especificat per minimitzar la combinació de les funcions de pèrdua de classificació i regressió durant el procés d'entrenament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1e7058",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(opt, classloss, regressloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3b8c9f",
   "metadata": {},
   "source": [
    "### 3.2 Entrenament\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d11d3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir='logs' #Crea un directori on es guardarà la informació del Tensorboard "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c141568",
   "metadata": {},
   "source": [
    "TensorBoard és una eina de visualització interactiva que s'utilitza en l'entrenament de models de xarxes neuronals per poder entendre millor el comportament del model durant l'entrenament i ajustar els paràmetres de manera més efectiva. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8616a537",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir) #crea un callback per registrar la informació del model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2666bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train, epochs=10, validation_data=val, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d0ac73",
   "metadata": {},
   "source": [
    "Aquesta última línia es passa tota la informació de train (osigui crida a tota la funció d'entrenament) per 10 iteracions (epochs) i registra les dades d'entrenament i validació durant l'entrenament a partir del callback de TensorBoard, llavors \"hist\" conté tota la info sobre les losses (pèrdues) i l'exactitud del model durant l'entrenament"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f34d80",
   "metadata": {},
   "source": [
    "### 3.3 Anàlisi del rendiment de l'entrenament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20d354a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history #per veure totes les pèrdues (losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bf7e81",
   "metadata": {},
   "source": [
    "El següent fragment mostra totes les losses de manera gràfica, en teoria tant les pèrdues i les pèrdues validades com les de classificació de localització i les totals haurien de ser valors molt semblants per a ser més exactes, en cas que no ho son deu haver-hi algun problema amb alguna anotation i no l'haurà processat bé (tampoc és un problema greu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb61c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(20,5))\n",
    "\n",
    "ax[0].plot(hist.history['total_loss'], color='teal', label='loss')\n",
    "ax[0].plot(hist.history['val_total_loss'], color='orange', label='val loss')\n",
    "ax[0].title.set_text('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(hist.history['class_loss'], color='teal', label='class loss')\n",
    "ax[1].plot(hist.history['val_class_loss'], color='orange', label='val class loss')\n",
    "ax[1].title.set_text('Classification Loss')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].plot(hist.history['regress_loss'], color='teal', label='regress loss')\n",
    "ax[2].plot(hist.history['val_regress_loss'], color='orange', label='val regress loss')\n",
    "ax[2].title.set_text('Regression Loss')\n",
    "ax[2].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5669ad25",
   "metadata": {},
   "source": [
    "### Provem i desem el model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4561a424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96958bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Detector.save(\"detector_fruites.h5\") #el desem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c4c925",
   "metadata": {},
   "source": [
    "### El provem amb la webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d2d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Detector = load_model('detector_fruites.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7accfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(1)\n",
    "while cap.isOpened():\n",
    "    _ , frame = cap.read()\n",
    "    frame = frame[50:500, 50:500,:]\n",
    "    \n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    resized = tf.image.resize(rgb, (120,120))\n",
    "    \n",
    "    yhat = Detector.predict(np.expand_dims(resized/255,0))\n",
    "    sample_coords = yhat[1][0]\n",
    "    \n",
    "    if yhat[0] > 0.5: \n",
    "        # Controls the main rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.multiply(sample_coords[:2], [450,450]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [450,450]).astype(int)), \n",
    "                            (255,0,0), 2)\n",
    "        # Controls the label rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int), \n",
    "                                    [0,-30])),\n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                    [80,0])), \n",
    "                            (255,0,0), -1)\n",
    "        \n",
    "        if yhat[0] > 0.8 and yhat[0] < 1.2:\n",
    "        # Controls the text rendered\n",
    "            cv2.putText(frame, 'poma', tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                                [0,-5])),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        elif yhat[0] > 1.8 and yhat[0] < 2.2:\n",
    "            cv2.putText(frame, 'pera', tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                                [0,-5])),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        elif yhat[0] > 2.8 and yhat[0] < 3.2:\n",
    "                        cv2.putText(frame, 'mandarina', tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                                [0,-5])),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        else:           \n",
    "                cv2.putText(frame, 'no ho tinc clar', tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                                [0,-5])),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    cv2.imshow('EyeTrack', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
